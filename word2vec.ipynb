{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_review = pd.read_pickle('yelp_review_cleaned_0710v3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>lemma_clean</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>count_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0W4lkclzZThpx3V65bVgig</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>v0i_UHJMo_hPBq9bxWvW4w</td>\n",
       "      <td>5</td>\n",
       "      <td>Love the staff, love the meat, love the place....</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>[love, the, staff, love, the, meat, love, the,...</td>\n",
       "      <td>6</td>\n",
       "      <td>[love the staff love the meat love the place p...</td>\n",
       "      <td>[(a, 3), (and, 1), (around, 1), (ask, 1), (can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>[super, simple, place, but, amazing, nonethele...</td>\n",
       "      <td>3</td>\n",
       "      <td>[super simple place but amazing nonetheless PR...</td>\n",
       "      <td>[(30, 1), (a, 1), (amazing, 1), (and, 3), (aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  0W4lkclzZThpx3V65bVgig     0 2016-05-28      0  v0i_UHJMo_hPBq9bxWvW4w   \n",
       "1  AEx2SYEUJmTxVVB18LlCwA     0 2016-05-28      0  vkVSCC7xljjrAI4UGfnKEQ   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      5  Love the staff, love the meat, love the place....       0   \n",
       "1      5  Super simple place but amazing nonetheless. It...       0   \n",
       "\n",
       "                  user_id                                        lemma_clean  \\\n",
       "0  bv2nCi5Qv5vroFiqKGopiw  [love, the, staff, love, the, meat, love, the,...   \n",
       "1  bv2nCi5Qv5vroFiqKGopiw  [super, simple, place, but, amazing, nonethele...   \n",
       "\n",
       "   sent_count                                         lemmatized  \\\n",
       "0           6  [love the staff love the meat love the place p...   \n",
       "1           3  [super simple place but amazing nonetheless PR...   \n",
       "\n",
       "                                           count_vec  \n",
       "0  [(a, 3), (and, 1), (around, 1), (ask, 1), (can...  \n",
       "1  [(30, 1), (a, 1), (amazing, 1), (and, 3), (aro...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_review.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review1 = df_review[df_review['stars'] == 1]['lemma_clean']\n",
    "review2 = df_review[df_review['stars'] == 5]['lemma_clean']\n",
    "review1_2 = pd.concat([review1,review2])\n",
    "# better to train word2vec in with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 18s, sys: 4.11 s, total: 1min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%time model = gensim.models.KeyedVectors.load_word2vec_format(\\\n",
    "                        '/home/vagrant/word2vec-package/GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model[model.wv.vocab][:10]\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# how to load the stanford pretrained word2vec\n",
    "# http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    w2v = {line.split()[0]: np.array(map(float, line.split()[1:]))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = gensim.models.word2vec.Word2Vec(review1_2,\n",
    "                                        size = 200, \n",
    "                                        window = 5, \n",
    "                                        min_count = 5,\n",
    "                                        workers = 4,\n",
    "                                        hs = 1,\n",
    "                                        negative = 0)\n",
    "\n",
    "#model.most_similar('happy')\n",
    "#model.doesnt_match(['king','queen','prince','Mike'])\n",
    "#model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model2 = gensim.models.word2vec.Word2Vec(review1_2,\n",
    "#                                         size = 200, # bigger data, bigger size, better model\n",
    "#                                         window = 5, \n",
    "#                                         min_count = 5,\n",
    "#                                         workers = 4, # only matter with Cython\n",
    "#                                         hs = 1,\n",
    "#                                         negative = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model1['phenomenal']) # raw NumPy vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'loyal', 0.48347151279449463),\n",
       " (u'relation', 0.4748350977897644),\n",
       " (u'satisfaction', 0.4667649567127228),\n",
       " (u'workmanship', 0.43191635608673096),\n",
       " (u'turnaround', 0.4208248257637024),\n",
       " (u'sevice', 0.4205753803253174),\n",
       " (u'value', 0.41442200541496277),\n",
       " (u'appreciation', 0.38947513699531555),\n",
       " (u'waitstaff', 0.38690808415412903),\n",
       " (u'elise', 0.376765638589859)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.most_similar('service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can store/load models using the standard gensim methods:\n",
    "# model.save('/tmp/mymodel')\n",
    "# new_model = gensim.models.Word2Vec.load('/tmp/mymodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features from word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "zz = zip(model1.wv.index2word, model.wv.syn0)\n",
    "# model.wv.vocab gives a dictionary of words and their vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cp/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# zip all the words and their vectors from word2vec\n",
    "w2v = dict(zip(model1.wv.index2word, model1.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # import pdb; pdb.set_trace()\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# TRY: XGBOOST classifer!!!\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "etree_w2v_tfidf = Pipeline([\n",
    "    (\"word2vec vectorizer\", TfidfEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", ExtraTreesClassifier(n_estimators=200))])\n",
    "\n",
    "etree_w2v_rf = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", RandomForestClassifier(n_estimators=20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(w2v)),\n",
    "    (\"extra trees\", SVC())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars1 = df_review[df_review['stars'] == 1]\n",
    "stars2 = df_review[df_review['stars'] == 5]\n",
    "stars3 = df_review[df_review['stars'] == 3]\n",
    "train_size = 0.75\n",
    "test_size = 1 - train_size\n",
    "\n",
    "df_train = pd.concat([stars1[:10000],stars2[:10000]])\n",
    "df_test = pd.concat([stars1[10000:len(stars1)],stars2[10000:len(stars1)]])\n",
    "\n",
    "\n",
    "#df_train = pd.concat([stars1[:int(len(stars1)*train_size)],stars2[:int(len(stars2)*train_size)]])\n",
    "#df_test = pd.concat([stars1[int(-len(stars1)*test_size):],stars2[int(-len(stars2)*test_size):]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train['stars'].apply(lambda x: 0 if x ==1 else 1)\n",
    "x_train = df_train['lemma_clean']\n",
    "y_test = df_test['stars'].apply(lambda x: 0 if x ==1 else 1)\n",
    "x_test = df_test['lemma_clean']\n",
    "x_test_3 = stars3['lemma_clean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('word2vec vectorizer', <__main__.MeanEmbeddingVectorizer object at 0x7f917c068fd0>), ('extra trees', ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impur...imators=200, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree_w2v.fit(x_train.values, y_train.values)\n",
    "# etree_w2v_tfidf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99955"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree_w2v_rf.fit(x_train.values, y_train.values)\n",
    "etree_w2v_rf.score(x_train.values, y_train.values)\n",
    "etree_w2v_rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91195\n",
      "0.9078665442301805\n"
     ]
    }
   ],
   "source": [
    "svc_w2v.fit(x_train.values, y_train.values)\n",
    "print svc_w2v.score(x_train.values, y_train.values)\n",
    "print svc_w2v.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13    [PRON, think, tidy, s, flower, have, a, great,...\n",
       "17    [PRON, too, have, be, try, to, book, an, appt,...\n",
       "20    [really, excited, to, hear, of, this, restaura...\n",
       "24    [food, be, very, bland, not, authentic, at, al...\n",
       "32    [if, PRON, have, not, yet, try, wasabi, do, no...\n",
       "44    [terrible, service, and, not, so, great, drink...\n",
       "45    [bad, customer, service, ever, \\n, manager, on...\n",
       "61    [after, read, the, review, on, yelp, PRON, boy...\n",
       "62    [come, here, for, a, friend, s, birthday, PRON...\n",
       "74    [after, read, review, on, this, place, think, ...\n",
       "Name: lemma_clean, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99995"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree_w2v.score(x_train.values, y_train.values)\n",
    "# check the shape of training data and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etree_w2v_tfidf.score(x_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8798591980410162"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etree_w2v.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6986\n",
       "0    4821\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(etree_w2v.predict(x_test_3)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    10000\n",
       "1    10000\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5916828999745913"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6986.0/(4821+6986)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.00000\n",
       "mean         3.00000\n",
       "std          2.00005\n",
       "min          1.00000\n",
       "25%          1.00000\n",
       "50%          3.00000\n",
       "75%          5.00000\n",
       "max          5.00000\n",
       "Name: stars, dtype: float64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyCP",
   "language": "python",
   "name": "cp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
